diff --git a/drivers/cpufreq/cpufreq_elementalx.c b/drivers/cpufreq/cpufreq_elementalx.c
new file mode 100644
index 00000000..3fc48972
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_elementalx.c
@@ -0,0 +1,562 @@
+/*
+ *  drivers/cpufreq/cpufreq_elementalx.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2015 Aaron Segaert <asegaert@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/slab.h>
+#include "cpufreq_governor.h"
+
+/* elementalx governor macros */
+#define DEF_FREQUENCY_UP_THRESHOLD		(90)
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(20)
+#define DEF_ACTIVE_FLOOR_FREQ			(960000)
+#define MIN_SAMPLING_RATE			(10000)
+#define DEF_SAMPLING_DOWN_FACTOR		(4)
+#define MAX_SAMPLING_DOWN_FACTOR		(20)
+#define FREQ_NEED_BURST(x)			(x < 800000 ? 1 : 0)
+#define MAX(x,y)				(x > y ? x : y)
+#define MIN(x,y)				(x < y ? x : y)
+#define TABLE_SIZE				12
+#define TABLE_NUM				6
+
+static DEFINE_PER_CPU(struct ex_cpu_dbs_info_s, ex_cpu_dbs_info);
+
+static DEFINE_PER_CPU(struct ex_dbs_tuners *, cached_tuners);
+
+static unsigned int up_threshold_level[2] __read_mostly = {95, 85};
+ 
+static struct ex_governor_data {
+	unsigned int active_floor_freq;
+	unsigned int prev_load;
+} ex_data = {
+	.active_floor_freq = DEF_ACTIVE_FLOOR_FREQ,
+	.prev_load = 0,
+};
+
+static unsigned int tblmap[TABLE_NUM][TABLE_SIZE] __read_mostly = {
+
+	//table 0
+	{
+		616400,
+		757200,
+		840000,
+		960000,
+		1248000,
+		1344000,
+		1478400,
+		1555200,
+		1632000,
+		1728000,
+		1824000,
+		1958400,
+	},
+
+	//table 1
+	{
+		773040,
+		899760,
+		1014960,
+		1072560,
+		1248000,
+		1344000,
+		1478400,
+		1555200,
+		1632000,
+		1728000,
+		1824000,
+		1958400,
+	},
+
+	//table 2
+	{
+		851100,
+		956700,
+		1052700,
+		1100700,
+		1350400,
+		1416000,
+		1550400,
+		1627200,
+		1740800,
+		1824000,
+		1920000,
+		2054400,
+	},
+
+	//table 3
+	{
+		616400,
+		757200,
+		840000,
+		960000,
+		1248000,
+		1344000,
+		1478400,
+		1555200,
+		1555200,
+		1555200,
+		1555200,
+		1555200,
+	},
+
+	//table 4
+	{
+		773040,
+		899760,
+		1014960,
+		1072560,
+		1248000,
+		1344000,
+		1478400,
+		1555200,
+		1555200,
+		1555200,
+		1555200,
+		1555200,
+	},
+
+	//table 5
+	{
+		851100,
+		956700,
+		1052700,
+		1100700,
+		1350400,
+		1416000,
+		1550400,
+		1627200,
+		1627200,
+		1627200,
+		1627200,
+		1627200,
+	}
+
+};
+
+static inline int get_cpu_freq_index(unsigned int freq, struct dbs_data *dbs_data)
+{
+	static int saved_index = 0;
+	int index;
+	
+	if (!dbs_data->freq_table) {
+		pr_warn("tbl is NULL, use previous value %d\n", saved_index);
+		return saved_index;
+	}
+
+	for (index = 0; (dbs_data->freq_table[index].frequency != CPUFREQ_TABLE_END); index++) {
+		if (dbs_data->freq_table[index].frequency >= freq) {
+			saved_index = index;
+			break;
+		}
+	}
+
+	return index;
+}
+
+static inline unsigned int ex_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (freq > p->max) {
+		return p->max;
+	} 
+	
+	return freq;
+}
+
+static void ex_check_cpu(int cpu, unsigned int load)
+{
+	struct ex_cpu_dbs_info_s *dbs_info = &per_cpu(ex_cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy = dbs_info->cdbs.cur_policy;
+	struct dbs_data *dbs_data = policy->governor_data;
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int max_load_freq = 0, freq_next = 0;
+	unsigned int j, avg_load, cur_freq, max_freq, target_freq = 0;
+
+	cur_freq = policy->cur;
+	max_freq = policy->max;
+
+	for_each_cpu(j, policy->cpus) {
+		if (load > max_load_freq)
+			max_load_freq = load * policy->cur;
+	}
+	avg_load = (ex_data.prev_load + load) >> 1;
+
+	if (max_load_freq > up_threshold_level[1] * cur_freq) {
+		int index = get_cpu_freq_index(cur_freq, dbs_data);
+
+		if (FREQ_NEED_BURST(cur_freq) &&
+				load > up_threshold_level[0]) {
+			freq_next = max_freq;
+		}
+		
+		else if (avg_load > up_threshold_level[0]) {
+			freq_next = tblmap[2 + ex_tuners->powersave][index];
+		}
+		
+		else if (avg_load <= up_threshold_level[1]) {
+			freq_next = tblmap[0 + ex_tuners->powersave][index];
+		}
+	
+		else {
+			if (load > up_threshold_level[0]) {
+				freq_next = tblmap[2 + ex_tuners->powersave][index];
+			}
+		
+			else {
+				freq_next = tblmap[1 + ex_tuners->powersave][index];
+			}
+		}
+
+		target_freq = ex_freq_increase(policy, freq_next);
+
+		__cpufreq_driver_target(policy, target_freq, CPUFREQ_RELATION_H);
+
+		if (target_freq > ex_data.active_floor_freq)
+			dbs_info->down_floor = 0;
+
+		goto finished;
+	}
+
+	if (cur_freq == policy->min)
+		goto finished;
+
+	if (cur_freq >= ex_data.active_floor_freq) {
+		if (++dbs_info->down_floor > ex_tuners->sampling_down_factor)
+			dbs_info->down_floor = 0;
+	} else {
+		dbs_info->down_floor = 0;
+	}
+
+	if (max_load_freq <
+	    (ex_tuners->up_threshold - ex_tuners->down_differential) *
+	     cur_freq) {
+
+		freq_next = max_load_freq /
+				(ex_tuners->up_threshold -
+				 ex_tuners->down_differential);
+
+		if (dbs_info->down_floor) {
+			freq_next = MAX(freq_next, ex_data.active_floor_freq);
+		} else {
+			freq_next = MAX(freq_next, policy->min);
+			if (freq_next < ex_data.active_floor_freq)
+				dbs_info->down_floor = ex_tuners->sampling_down_factor;
+		}
+
+		__cpufreq_driver_target(policy, freq_next,
+			CPUFREQ_RELATION_L);
+	}
+
+finished:
+	ex_data.prev_load = load;
+	return;
+}
+
+static void ex_dbs_timer(struct work_struct *work)
+{
+	struct ex_cpu_dbs_info_s *dbs_info = container_of(work,
+			struct ex_cpu_dbs_info_s, cdbs.work.work);
+	unsigned int cpu = dbs_info->cdbs.cur_policy->cpu;
+	struct ex_cpu_dbs_info_s *core_dbs_info = &per_cpu(ex_cpu_dbs_info,
+			cpu);
+	struct dbs_data *dbs_data = dbs_info->cdbs.cur_policy->governor_data;
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	int delay = delay_for_sampling_rate(ex_tuners->sampling_rate);
+	bool modify_all = true;
+
+	mutex_lock(&core_dbs_info->cdbs.timer_mutex);
+	if (!need_load_eval(&core_dbs_info->cdbs, ex_tuners->sampling_rate))
+		modify_all = false;
+	else
+		dbs_check_cpu(dbs_data, cpu);
+
+	gov_queue_work(dbs_data, dbs_info->cdbs.cur_policy, delay, modify_all);
+	mutex_unlock(&core_dbs_info->cdbs.timer_mutex);
+}
+
+/************************** sysfs interface ************************/
+static struct common_dbs_data ex_dbs_cdata;
+
+static ssize_t store_sampling_rate(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	ex_tuners->sampling_rate = max(input, dbs_data->min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input <= ex_tuners->down_differential)
+		return -EINVAL;
+
+	ex_tuners->up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input >= ex_tuners->up_threshold)
+		return -EINVAL;
+
+	ex_tuners->down_differential = input;
+	return count;
+}
+
+static ssize_t store_active_floor_freq(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	ex_tuners->active_floor_freq = input;
+	ex_data.active_floor_freq = ex_tuners->active_floor_freq;
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 0)
+		return -EINVAL;
+
+	ex_tuners->sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_powersave(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 1)
+		return -EINVAL;
+
+	if (input == 0)
+		ex_tuners->powersave = input;
+	else if (input == 1)
+		ex_tuners->powersave = 3;
+
+	return count;
+}
+
+show_store_one(ex, sampling_rate);
+show_store_one(ex, up_threshold);
+show_store_one(ex, down_differential);
+show_store_one(ex, active_floor_freq);
+show_store_one(ex, sampling_down_factor);
+show_store_one(ex, powersave);
+declare_show_sampling_rate_min(ex);
+
+gov_sys_pol_attr_rw(sampling_rate);
+gov_sys_pol_attr_rw(up_threshold);
+gov_sys_pol_attr_rw(down_differential);
+gov_sys_pol_attr_rw(active_floor_freq);
+gov_sys_pol_attr_rw(sampling_down_factor);
+gov_sys_pol_attr_rw(powersave);
+gov_sys_pol_attr_ro(sampling_rate_min);
+
+static struct attribute *dbs_attributes_gov_sys[] = {
+	&sampling_rate_min_gov_sys.attr,
+	&sampling_rate_gov_sys.attr,
+	&up_threshold_gov_sys.attr,
+	&down_differential_gov_sys.attr,
+	&active_floor_freq_gov_sys.attr,
+	&sampling_down_factor_gov_sys.attr,
+	&powersave_gov_sys.attr,
+	NULL
+};
+
+static struct attribute_group ex_attr_group_gov_sys = {
+	.attrs = dbs_attributes_gov_sys,
+	.name = "elementalx",
+};
+
+static struct attribute *dbs_attributes_gov_pol[] = {
+	&sampling_rate_min_gov_pol.attr,
+	&sampling_rate_gov_pol.attr,
+	&up_threshold_gov_pol.attr,
+	&down_differential_gov_pol.attr,
+	&active_floor_freq_gov_pol.attr,
+	&sampling_down_factor_gov_pol.attr,
+	&powersave_gov_pol.attr,
+	NULL
+};
+
+static struct attribute_group ex_attr_group_gov_pol = {
+	.attrs = dbs_attributes_gov_pol,
+	.name = "elementalx",
+};
+
+/************************** sysfs end ************************/
+
+static void save_tuners(struct cpufreq_policy *policy,
+			  struct ex_dbs_tuners *tuners)
+{
+	int cpu;
+
+	if (have_governor_per_policy())
+		cpu = cpumask_first(policy->related_cpus);
+	else
+		cpu = 0;
+
+	WARN_ON(per_cpu(cached_tuners, cpu) &&
+		per_cpu(cached_tuners, cpu) != tuners);
+	per_cpu(cached_tuners, cpu) = tuners;
+}
+
+static struct ex_dbs_tuners *alloc_tuners(struct cpufreq_policy *policy)
+{
+	struct ex_dbs_tuners *tuners;
+
+	tuners = kzalloc(sizeof(*tuners), GFP_KERNEL);
+	if (!tuners) {
+		pr_err("%s: kzalloc failed\n", __func__);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	tuners->up_threshold = DEF_FREQUENCY_UP_THRESHOLD;
+	tuners->down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL;
+	tuners->ignore_nice_load = 0;
+	tuners->active_floor_freq = DEF_ACTIVE_FLOOR_FREQ;
+	tuners->sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR;
+	tuners->powersave = 0;
+
+	save_tuners(policy, tuners);
+
+	return tuners;
+}
+
+static struct ex_dbs_tuners *restore_tuners(struct cpufreq_policy *policy)
+{
+	int cpu;
+
+	if (have_governor_per_policy())
+		cpu = cpumask_first(policy->related_cpus);
+	else
+		cpu = 0;
+
+	return per_cpu(cached_tuners, cpu);
+}
+
+static int ex_init(struct dbs_data *dbs_data, struct cpufreq_policy *policy)
+{
+	struct ex_dbs_tuners *tuners;
+
+	tuners = restore_tuners(policy);
+	if (!tuners) {
+		tuners = alloc_tuners(policy);
+		if (IS_ERR(tuners))
+			return PTR_ERR(tuners);
+	}
+
+	dbs_data->tuners = tuners;
+	dbs_data->min_sampling_rate = MIN_SAMPLING_RATE;
+	dbs_data->freq_table = cpufreq_frequency_get_table(policy->cpu);
+
+	mutex_init(&dbs_data->mutex);
+
+	return 0;
+}
+
+static void ex_exit(struct dbs_data *dbs_data)
+{
+	//nothing to do
+}
+
+define_get_cpu_dbs_routines(ex_cpu_dbs_info);
+
+static struct common_dbs_data ex_dbs_cdata = {
+	.governor = GOV_ELEMENTALX,
+	.attr_group_gov_sys = &ex_attr_group_gov_sys,
+	.attr_group_gov_pol = &ex_attr_group_gov_pol,
+	.get_cpu_cdbs = get_cpu_cdbs,
+	.get_cpu_dbs_info_s = get_cpu_dbs_info_s,
+	.gov_dbs_timer = ex_dbs_timer,
+	.gov_check_cpu = ex_check_cpu,
+	.init_ex = ex_init,
+	.exit = ex_exit,
+};
+
+static int ex_cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	return cpufreq_governor_dbs(policy, &ex_dbs_cdata, event);
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ELEMENTALX
+static
+#endif
+struct cpufreq_governor cpufreq_gov_elementalx = {
+	.name			= "elementalx",
+	.governor		= ex_cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_elementalx);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	int cpu;
+
+	cpufreq_unregister_governor(&cpufreq_gov_elementalx);
+	for_each_possible_cpu(cpu) {
+		kfree(per_cpu(cached_tuners, cpu));
+		per_cpu(cached_tuners, cpu) = NULL;
+	}
+
+}
+
+MODULE_AUTHOR("Aaron Segaert <asegaert@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_elementalx' - multiphase cpufreq governor");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ELEMENTALX
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_nightmare.c b/drivers/cpufreq/cpufreq_nightmare.c
new file mode 100644
index 00000000..fe8cc86b
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_nightmare.c
@@ -0,0 +1,871 @@
+/*
+ *  drivers/cpufreq/cpufreq_nightmare.c
+ *
+ *  Copyright (C)  2011 Samsung Electronics co. ltd
+ *    ByungChang Cha <bc.cha@samsung.com>
+ *
+ *  Based on ondemand governor
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ * 
+ * Created by Alucard_24@xda
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+static void do_nightmare_timer(struct work_struct *work);
+static int cpufreq_governor_nightmare(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_nightmare = {
+	.name                   = "nightmare",
+	.governor               = cpufreq_governor_nightmare,
+	.owner                  = THIS_MODULE,
+};
+
+struct cpufreq_nightmare_cpuinfo {
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_idle;
+	struct cpufreq_frequency_table *freq_table;
+	struct delayed_work work;
+	struct cpufreq_policy *cur_policy;
+	int cpu;
+	unsigned int enable:1;
+	/*
+	 * mutex that serializes governor limit change with
+	 * do_nightmare_timer invocation. We do not want do_nightmare_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_nightmare_cpuinfo, od_nightmare_cpuinfo);
+
+static unsigned int nightmare_enable;	/* number of CPUs using this policy */
+/*
+ * nightmare_mutex protects nightmare_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(nightmare_mutex);
+
+/*static atomic_t min_freq_limit[NR_CPUS];
+static atomic_t max_freq_limit[NR_CPUS];*/
+
+/* nightmare tuners */
+static struct nightmare_tuners {
+	atomic_t sampling_rate;
+	atomic_t inc_cpu_load_at_min_freq;
+	atomic_t inc_cpu_load;
+	atomic_t dec_cpu_load;
+	atomic_t freq_for_responsiveness;
+	atomic_t freq_for_responsiveness_max;
+	atomic_t freq_up_brake_at_min_freq;
+	atomic_t freq_up_brake;
+	atomic_t freq_step_at_min_freq;
+	atomic_t freq_step;
+	atomic_t freq_step_dec;
+	atomic_t freq_step_dec_at_max_freq;
+#ifdef CONFIG_CPU_EXYNOS4210
+	atomic_t up_sf_step;
+	atomic_t down_sf_step;
+#endif
+} nightmare_tuners_ins = {
+	.sampling_rate = ATOMIC_INIT(60000),
+	.inc_cpu_load_at_min_freq = ATOMIC_INIT(60),
+	.inc_cpu_load = ATOMIC_INIT(70),
+	.dec_cpu_load = ATOMIC_INIT(50),
+#ifdef CONFIG_CPU_EXYNOS4210
+	.freq_for_responsiveness = ATOMIC_INIT(200000),
+	.freq_for_responsiveness_max = ATOMIC_INIT(1200000),
+#else
+	.freq_for_responsiveness = ATOMIC_INIT(540000),
+	.freq_for_responsiveness_max = ATOMIC_INIT(1890000),
+#endif
+	.freq_step_at_min_freq = ATOMIC_INIT(20),
+	.freq_step = ATOMIC_INIT(20),
+	.freq_up_brake_at_min_freq = ATOMIC_INIT(30),
+	.freq_up_brake = ATOMIC_INIT(30),
+	.freq_step_dec = ATOMIC_INIT(10),
+	.freq_step_dec_at_max_freq = ATOMIC_INIT(10),
+#ifdef CONFIG_CPU_EXYNOS4210
+	.up_sf_step = ATOMIC_INIT(0),
+	.down_sf_step = ATOMIC_INIT(0),
+#endif
+};
+
+/************************** sysfs interface ************************/
+
+/* cpufreq_nightmare Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%d\n", atomic_read(&nightmare_tuners_ins.object));		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(inc_cpu_load_at_min_freq, inc_cpu_load_at_min_freq);
+show_one(inc_cpu_load, inc_cpu_load);
+show_one(dec_cpu_load, dec_cpu_load);
+show_one(freq_for_responsiveness, freq_for_responsiveness);
+show_one(freq_for_responsiveness_max, freq_for_responsiveness_max);
+show_one(freq_step_at_min_freq, freq_step_at_min_freq);
+show_one(freq_step, freq_step);
+show_one(freq_up_brake_at_min_freq, freq_up_brake_at_min_freq);
+show_one(freq_up_brake, freq_up_brake);
+show_one(freq_step_dec, freq_step_dec);
+show_one(freq_step_dec_at_max_freq, freq_step_dec_at_max_freq);
+#ifdef CONFIG_CPU_EXYNOS4210
+show_one(up_sf_step, up_sf_step);
+show_one(down_sf_step, down_sf_step);
+#endif
+
+/*#define show_freqlimit_param(file_name, cpu)		\
+static ssize_t show_##file_name##_##cpu		\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%d\n", atomic_read(&file_name[cpu]));	\
+}
+
+#define store_freqlimit_param(file_name, cpu)		\
+static ssize_t store_##file_name##_##cpu		\
+(struct kobject *kobj, struct attribute *attr,				\
+	const char *buf, size_t count)					\
+{									\
+	unsigned int input;						\
+	int ret;							\
+	ret = sscanf(buf, "%d", &input);				\
+	if (ret != 1)							\
+		return -EINVAL;						\
+	if (input == atomic_read(&file_name[cpu])) {		\
+		return count;	\
+	}	\
+	atomic_set(&file_name[cpu], input);			\
+	return count;							\
+}*/
+
+/* min freq limit for awaking */
+/*show_freqlimit_param(min_freq_limit, 0);
+show_freqlimit_param(min_freq_limit, 1);
+#if NR_CPUS >= 4
+show_freqlimit_param(min_freq_limit, 2);
+show_freqlimit_param(min_freq_limit, 3);
+#endif*/
+/* max freq limit for awaking */
+/*show_freqlimit_param(max_freq_limit, 0);
+show_freqlimit_param(max_freq_limit, 1);
+#if NR_CPUS >= 4
+show_freqlimit_param(max_freq_limit, 2);
+show_freqlimit_param(max_freq_limit, 3);
+#endif*/
+/* min freq limit for awaking */
+/*store_freqlimit_param(min_freq_limit, 0);
+store_freqlimit_param(min_freq_limit, 1);
+#if NR_CPUS >= 4
+store_freqlimit_param(min_freq_limit, 2);
+store_freqlimit_param(min_freq_limit, 3);
+#endif*/
+/* max freq limit for awaking */
+/*store_freqlimit_param(max_freq_limit, 0);
+store_freqlimit_param(max_freq_limit, 1);
+#if NR_CPUS >= 4
+store_freqlimit_param(max_freq_limit, 2);
+store_freqlimit_param(max_freq_limit, 3);
+#endif
+define_one_global_rw(min_freq_limit_0);
+define_one_global_rw(min_freq_limit_1);
+#if NR_CPUS >= 4
+define_one_global_rw(min_freq_limit_2);
+define_one_global_rw(min_freq_limit_3);
+#endif
+define_one_global_rw(max_freq_limit_0);
+define_one_global_rw(max_freq_limit_1);
+#if NR_CPUS >= 4
+define_one_global_rw(max_freq_limit_2);
+define_one_global_rw(max_freq_limit_3);
+#endif*/
+
+/**
+ * update_sampling_rate - update sampling rate effective immediately if needed.
+ * @new_rate: new sampling rate
+ *
+ * If new rate is smaller than the old, simply updaing
+ * nightmare_tuners_ins.sampling_rate might not be appropriate. For example,
+ * if the original sampling_rate was 1 second and the requested new sampling
+ * rate is 10 ms because the user needs immediate reaction from ondemand
+ * governor, but not sure if higher frequency will be required or not,
+ * then, the governor may change the sampling rate too late; up to 1 second
+ * later. Thus, if we are reducing the sampling rate, we need to make the
+ * new value effective immediately.
+ */
+static void update_sampling_rate(unsigned int new_rate)
+{
+	int cpu;
+
+	atomic_set(&nightmare_tuners_ins.sampling_rate,new_rate);
+
+	get_online_cpus();
+	for_each_online_cpu(cpu) {
+		struct cpufreq_policy *policy;
+		struct cpufreq_nightmare_cpuinfo *nightmare_cpuinfo;
+		unsigned long next_sampling, appointed_at;
+
+		policy = cpufreq_cpu_get(cpu);
+		if (!policy)
+			continue;
+		nightmare_cpuinfo = &per_cpu(od_nightmare_cpuinfo, policy->cpu);
+		cpufreq_cpu_put(policy);
+
+		mutex_lock(&nightmare_cpuinfo->timer_mutex);
+
+		if (!delayed_work_pending(&nightmare_cpuinfo->work)) {
+			mutex_unlock(&nightmare_cpuinfo->timer_mutex);
+			continue;
+		}
+
+		next_sampling  = jiffies + usecs_to_jiffies(new_rate);
+		appointed_at = nightmare_cpuinfo->work.timer.expires;
+
+
+		if (time_before(next_sampling, appointed_at)) {
+
+			mutex_unlock(&nightmare_cpuinfo->timer_mutex);
+			cancel_delayed_work_sync(&nightmare_cpuinfo->work);
+			mutex_lock(&nightmare_cpuinfo->timer_mutex);
+
+			#ifdef CONFIG_CPU_EXYNOS4210
+				mod_delayed_work_on(nightmare_cpuinfo->cpu, system_wq, &nightmare_cpuinfo->work, usecs_to_jiffies(new_rate));
+			#else
+				queue_delayed_work_on(nightmare_cpuinfo->cpu, system_wq, &nightmare_cpuinfo->work, usecs_to_jiffies(new_rate));
+			#endif
+		}
+		mutex_unlock(&nightmare_cpuinfo->timer_mutex);
+	}
+	put_online_cpus();
+}
+
+/* sampling_rate */
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input,10000);
+	
+	if (input == atomic_read(&nightmare_tuners_ins.sampling_rate))
+		return count;
+
+	update_sampling_rate(input);
+
+	return count;
+}
+
+/* inc_cpu_load_at_min_freq */
+static ssize_t store_inc_cpu_load_at_min_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1) {
+		return -EINVAL;
+	}
+
+	input = min(input,atomic_read(&nightmare_tuners_ins.inc_cpu_load));
+
+	if (input == atomic_read(&nightmare_tuners_ins.inc_cpu_load_at_min_freq))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.inc_cpu_load_at_min_freq,input);
+
+	return count;
+}
+
+/* inc_cpu_load */
+static ssize_t store_inc_cpu_load(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.inc_cpu_load))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.inc_cpu_load,input);
+
+	return count;
+}
+
+/* dec_cpu_load */
+static ssize_t store_dec_cpu_load(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,95),5);
+
+	if (input == atomic_read(&nightmare_tuners_ins.dec_cpu_load))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.dec_cpu_load,input);
+
+	return count;
+}
+
+/* freq_for_responsiveness */
+static ssize_t store_freq_for_responsiveness(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_for_responsiveness))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.freq_for_responsiveness,input);
+
+	return count;
+}
+
+/* freq_for_responsiveness_max */
+static ssize_t store_freq_for_responsiveness_max(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_for_responsiveness_max))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.freq_for_responsiveness_max,input);
+
+	return count;
+}
+
+/* freq_step_at_min_freq */
+static ssize_t store_freq_step_at_min_freq(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_step_at_min_freq))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.freq_step_at_min_freq,input);
+
+	return count;
+}
+
+/* freq_step */
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_step))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.freq_step,input);
+
+	return count;
+}
+
+/* freq_up_brake_at_min_freq */
+static ssize_t store_freq_up_brake_at_min_freq(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_up_brake_at_min_freq)) {/* nothing to do */
+		return count;
+	}
+
+	atomic_set(&nightmare_tuners_ins.freq_up_brake_at_min_freq,input);
+
+	return count;
+}
+
+/* freq_up_brake */
+static ssize_t store_freq_up_brake(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_up_brake)) {/* nothing to do */
+		return count;
+	}
+
+	atomic_set(&nightmare_tuners_ins.freq_up_brake,input);
+
+	return count;
+}
+
+/* freq_step_dec */
+static ssize_t store_freq_step_dec(struct kobject *a, struct attribute *b,
+				       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_step_dec)) {/* nothing to do */
+		return count;
+	}
+
+	atomic_set(&nightmare_tuners_ins.freq_step_dec,input);
+
+	return count;
+}
+
+/* freq_step_dec_at_max_freq */
+static ssize_t store_freq_step_dec_at_max_freq(struct kobject *a, struct attribute *b,
+				       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_step_dec_at_max_freq)) {/* nothing to do */
+		return count;
+	}
+
+	atomic_set(&nightmare_tuners_ins.freq_step_dec_at_max_freq,input);
+
+	return count;
+}
+#ifdef CONFIG_CPU_EXYNOS4210
+/* up_sf_step */
+static ssize_t store_up_sf_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,99),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.up_sf_step))
+		return count;
+
+	 atomic_set(&nightmare_tuners_ins.up_sf_step,input);
+
+	return count;
+}
+
+/* down_sf_step */
+static ssize_t store_down_sf_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,99),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.down_sf_step))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.down_sf_step,input);
+
+	return count;
+}
+#endif
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(inc_cpu_load_at_min_freq);
+define_one_global_rw(inc_cpu_load);
+define_one_global_rw(dec_cpu_load);
+define_one_global_rw(freq_for_responsiveness);
+define_one_global_rw(freq_for_responsiveness_max);
+define_one_global_rw(freq_step_at_min_freq);
+define_one_global_rw(freq_step);
+define_one_global_rw(freq_up_brake_at_min_freq);
+define_one_global_rw(freq_up_brake);
+define_one_global_rw(freq_step_dec);
+define_one_global_rw(freq_step_dec_at_max_freq);
+#ifdef CONFIG_CPU_EXYNOS4210
+define_one_global_rw(up_sf_step);
+define_one_global_rw(down_sf_step);
+#endif
+
+static struct attribute *nightmare_attributes[] = {
+	&sampling_rate.attr,
+	/*&min_freq_limit_0.attr,
+	&min_freq_limit_1.attr,
+#if NR_CPUS >= 4
+	&min_freq_limit_2.attr,
+	&min_freq_limit_3.attr,
+#endif
+	&max_freq_limit_0.attr,
+	&max_freq_limit_1.attr,
+#if NR_CPUS >= 4
+	&max_freq_limit_2.attr,
+	&max_freq_limit_3.attr,
+#endif*/
+	&inc_cpu_load_at_min_freq.attr,
+	&inc_cpu_load.attr,
+	&dec_cpu_load.attr,
+	&freq_for_responsiveness.attr,
+	&freq_for_responsiveness_max.attr,
+	&freq_step_at_min_freq.attr,
+	&freq_step.attr,
+	&freq_up_brake_at_min_freq.attr,
+	&freq_up_brake.attr,
+	&freq_step_dec.attr,
+	&freq_step_dec_at_max_freq.attr,
+#ifdef CONFIG_CPU_EXYNOS4210
+	&up_sf_step.attr,
+	&down_sf_step.attr,
+#endif
+	NULL
+};
+
+static struct attribute_group nightmare_attr_group = {
+	.attrs = nightmare_attributes,
+	.name = "nightmare",
+};
+
+/************************** sysfs end ************************/
+
+static void nightmare_check_cpu(struct cpufreq_nightmare_cpuinfo *this_nightmare_cpuinfo)
+{
+	struct cpufreq_policy *cpu_policy;
+	unsigned int min_freq;
+	unsigned int max_freq;
+#ifdef CONFIG_CPU_EXYNOS4210
+	int up_sf_step = atomic_read(&nightmare_tuners_ins.up_sf_step);
+	int down_sf_step = atomic_read(&nightmare_tuners_ins.down_sf_step);
+#endif
+	unsigned int freq_for_responsiveness;
+	unsigned int freq_for_responsiveness_max;
+	int dec_cpu_load;
+	int inc_cpu_load;
+	int freq_step;
+	int freq_up_brake;
+	int freq_step_dec;
+	cputime64_t cur_wall_time, cur_idle_time;
+	unsigned int wall_time, idle_time;
+	unsigned int index = 0;
+	unsigned int tmp_freq = 0;
+	unsigned int next_freq = 0;
+	int cur_load = -1;
+	unsigned int cpu;
+	
+	cpu = this_nightmare_cpuinfo->cpu;
+	cpu_policy = this_nightmare_cpuinfo->cur_policy;
+
+	cur_idle_time = get_cpu_idle_time_us(cpu, NULL);
+	cur_idle_time += get_cpu_iowait_time_us(cpu, &cur_wall_time);
+
+	wall_time = (unsigned int)
+			(cur_wall_time - this_nightmare_cpuinfo->prev_cpu_wall);
+	this_nightmare_cpuinfo->prev_cpu_wall = cur_wall_time;
+
+	idle_time = (unsigned int)
+			(cur_idle_time - this_nightmare_cpuinfo->prev_cpu_idle);
+	this_nightmare_cpuinfo->prev_cpu_idle = cur_idle_time;
+
+	/*min_freq = atomic_read(&min_freq_limit[cpu]);
+	max_freq = atomic_read(&max_freq_limit[cpu]);*/
+
+	freq_for_responsiveness = atomic_read(&nightmare_tuners_ins.freq_for_responsiveness);
+	freq_for_responsiveness_max = atomic_read(&nightmare_tuners_ins.freq_for_responsiveness_max);
+	dec_cpu_load = atomic_read(&nightmare_tuners_ins.dec_cpu_load);
+	inc_cpu_load = atomic_read(&nightmare_tuners_ins.inc_cpu_load);
+	freq_step = atomic_read(&nightmare_tuners_ins.freq_step);
+	freq_up_brake = atomic_read(&nightmare_tuners_ins.freq_up_brake);
+	freq_step_dec = atomic_read(&nightmare_tuners_ins.freq_step_dec);
+
+	if (!cpu_policy)
+		return;
+
+	/*printk(KERN_ERR "TIMER CPU[%u], wall[%u], idle[%u]\n",cpu, wall_time, idle_time);*/
+	if (wall_time >= idle_time) { /*if wall_time < idle_time, evaluate cpu load next time*/
+		cur_load = wall_time > idle_time ? (100 * (wall_time - idle_time)) / wall_time : 1;/*if wall_time is equal to idle_time cpu_load is equal to 1*/
+		tmp_freq = cpu_policy->cur;
+		/* Checking Frequency Limit */
+		/*if (max_freq > cpu_policy->max)
+			max_freq = cpu_policy->max;
+		if (min_freq < cpu_policy->min)
+			min_freq = cpu_policy->min;*/
+		min_freq = cpu_policy->min;
+		max_freq = cpu_policy->max;		
+		/* CPUs Online Scale Frequency*/
+		if (cpu_policy->cur < freq_for_responsiveness) {
+			inc_cpu_load = atomic_read(&nightmare_tuners_ins.inc_cpu_load_at_min_freq);
+			freq_step = atomic_read(&nightmare_tuners_ins.freq_step_at_min_freq);
+			freq_up_brake = atomic_read(&nightmare_tuners_ins.freq_up_brake_at_min_freq);
+		} else if (cpu_policy->cur > freq_for_responsiveness_max) {
+			freq_step_dec = atomic_read(&nightmare_tuners_ins.freq_step_dec_at_max_freq);
+		}		
+		/* Check for frequency increase or for frequency decrease */
+#ifdef CONFIG_CPU_EXYNOS4210
+		if (cur_load >= inc_cpu_load && cpu_policy->cur < max_freq) {
+			tmp_freq = max(min((cpu_policy->cur + ((cur_load + freq_step - freq_up_brake == 0 ? 1 : cur_load + freq_step - freq_up_brake) * 2000)), max_freq), min_freq);
+		} else if (cur_load < dec_cpu_load && cpu_policy->cur > min_freq) {
+			tmp_freq = max(min((cpu_policy->cur - ((100 - cur_load + freq_step_dec == 0 ? 1 : 100 - cur_load + freq_step_dec) * 2000)), max_freq), min_freq);
+		}
+		next_freq = (tmp_freq / 100000) * 100000;
+		if ((next_freq > cpu_policy->cur
+			&& (tmp_freq % 100000 > up_sf_step * 1000))
+			|| (next_freq < cpu_policy->cur
+			&& (tmp_freq % 100000 > down_sf_step * 1000))) {
+				next_freq += 100000;
+		}
+#else
+		if (cur_load >= inc_cpu_load && cpu_policy->cur < max_freq) {
+			tmp_freq = max(min((cpu_policy->cur + ((cur_load + freq_step - freq_up_brake == 0 ? 1 : cur_load + freq_step - freq_up_brake) * 3840)), max_freq), min_freq);
+		} else if (cur_load < dec_cpu_load && cpu_policy->cur > min_freq) {
+			tmp_freq = max(min((cpu_policy->cur - ((100 - cur_load + freq_step_dec == 0 ? 1 : 100 - cur_load + freq_step_dec) * 3840)), max_freq), min_freq);
+		}
+		cpufreq_frequency_table_target(cpu_policy, this_nightmare_cpuinfo->freq_table, tmp_freq,
+			CPUFREQ_RELATION_H, &index);
+		if (this_nightmare_cpuinfo->freq_table[index].frequency != cpu_policy->cur) {
+			cpufreq_frequency_table_target(cpu_policy, this_nightmare_cpuinfo->freq_table, tmp_freq,
+				CPUFREQ_RELATION_L, &index);
+		}
+	 	next_freq = this_nightmare_cpuinfo->freq_table[index].frequency;
+#endif
+		/*printk(KERN_ERR "FREQ CALC.: CPU[%u], load[%d], target freq[%u], cur freq[%u], min freq[%u], max_freq[%u]\n",cpu, cur_load, next_freq, cpu_policy->cur, cpu_policy->min, max_freq);*/
+		if (next_freq != cpu_policy->cur && cpu_online(cpu)) {
+			__cpufreq_driver_target(cpu_policy, next_freq, CPUFREQ_RELATION_L);
+		}
+	}
+
+}
+
+static void do_nightmare_timer(struct work_struct *work)
+{
+	struct cpufreq_nightmare_cpuinfo *nightmare_cpuinfo;
+	int delay;
+	unsigned int cpu;
+
+	nightmare_cpuinfo = container_of(work, struct cpufreq_nightmare_cpuinfo, work.work);
+	cpu = nightmare_cpuinfo->cpu;
+
+	mutex_lock(&nightmare_cpuinfo->timer_mutex);
+	nightmare_check_cpu(nightmare_cpuinfo);
+	/* We want all CPUs to do sampling nearly on
+	 * same jiffy
+	 */
+	delay = usecs_to_jiffies(atomic_read(&nightmare_tuners_ins.sampling_rate));
+	if (num_online_cpus() > 1) {
+		delay -= jiffies % delay;
+	}
+
+#ifdef CONFIG_CPU_EXYNOS4210
+	mod_delayed_work_on(cpu, system_wq, &nightmare_cpuinfo->work, delay);
+#else
+	queue_delayed_work_on(cpu, system_wq, &nightmare_cpuinfo->work, delay);
+#endif
+	mutex_unlock(&nightmare_cpuinfo->timer_mutex);
+}
+
+static int cpufreq_governor_nightmare(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	unsigned int cpu;
+	struct cpufreq_nightmare_cpuinfo *this_nightmare_cpuinfo;
+	int rc, delay;
+
+	cpu = policy->cpu;
+	this_nightmare_cpuinfo = &per_cpu(od_nightmare_cpuinfo, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&nightmare_mutex);
+
+		this_nightmare_cpuinfo->cur_policy = policy;
+
+		this_nightmare_cpuinfo->prev_cpu_idle = get_cpu_idle_time_us(cpu, NULL);
+		this_nightmare_cpuinfo->prev_cpu_idle += get_cpu_iowait_time_us(cpu, &this_nightmare_cpuinfo->prev_cpu_wall);
+
+		this_nightmare_cpuinfo->freq_table = cpufreq_frequency_get_table(cpu);
+		this_nightmare_cpuinfo->cpu = cpu;
+
+		mutex_init(&this_nightmare_cpuinfo->timer_mutex);
+
+		nightmare_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (nightmare_enable == 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&nightmare_attr_group);
+			if (rc) {
+				mutex_unlock(&nightmare_mutex);
+				return rc;
+			}
+		}
+
+		/*if (atomic_read(&min_freq_limit[cpu]) == 0)
+			atomic_set(&min_freq_limit[cpu], policy->min);
+
+		if (atomic_read(&max_freq_limit[cpu]) == 0)
+			atomic_set(&max_freq_limit[cpu], policy->max);*/
+
+		mutex_unlock(&nightmare_mutex);
+
+		delay=usecs_to_jiffies(atomic_read(&nightmare_tuners_ins.sampling_rate));
+		if (num_online_cpus() > 1) {
+			delay -= jiffies % delay;
+		}
+
+		this_nightmare_cpuinfo->enable = 1;
+/*#ifdef CONFIG_CPU_EXYNOS4210
+		INIT_DEFERRABLE_WORK(&this_nightmare_cpuinfo->work, do_nightmare_timer);
+		mod_delayed_work_on(this_nightmare_cpuinfo->cpu, system_wq, &this_nightmare_cpuinfo->work, delay);
+#else
+		INIT_DELAYED_WORK_DEFERRABLE(&this_nightmare_cpuinfo->work, do_nightmare_timer);
+		queue_delayed_work_on(this_nightmare_cpuinfo->cpu, system_wq, &this_nightmare_cpuinfo->work, delay);
+#endif*/
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		this_nightmare_cpuinfo->enable = 0;
+		cancel_delayed_work_sync(&this_nightmare_cpuinfo->work);
+
+		mutex_lock(&nightmare_mutex);
+		nightmare_enable--;
+		mutex_destroy(&this_nightmare_cpuinfo->timer_mutex);
+
+		if (!nightmare_enable) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &nightmare_attr_group);			
+		}
+		mutex_unlock(&nightmare_mutex);
+		
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_nightmare_cpuinfo->timer_mutex);
+		if (policy->max < this_nightmare_cpuinfo->cur_policy->cur)
+			__cpufreq_driver_target(this_nightmare_cpuinfo->cur_policy,
+				policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_nightmare_cpuinfo->cur_policy->cur)
+			__cpufreq_driver_target(this_nightmare_cpuinfo->cur_policy,
+				policy->min, CPUFREQ_RELATION_L);
+		mutex_unlock(&this_nightmare_cpuinfo->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+static int __init cpufreq_gov_nightmare_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_nightmare);
+}
+
+static void __exit cpufreq_gov_nightmare_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_nightmare);
+}
+
+MODULE_AUTHOR("Alucard24@XDA");
+MODULE_DESCRIPTION("'cpufreq_nightmare' - A dynamic cpufreq/cpuhotplug governor v4.1 (SnapDragon)");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE
+fs_initcall(cpufreq_gov_nightmare_init);
+#else
+module_init(cpufreq_gov_nightmare_init);
+#endif
+module_exit(cpufreq_gov_nightmare_exit);
+
diff --git a/drivers/cpufreq/cpufreq_zzmoove.c b/drivers/cpufreq/cpufreq_zzmoove.c
new file mode 100644
index 00000000..233ac3e7
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_zzmoove.c
@@ -0,0 +1,820 @@
+/*
+ *  drivers/cpufreq/cpufreq_zzmoove.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2009 Alexander Clouter <alex@digriz.org.uk>
+ *            (C)  2012 Michael Weingaertner <mialwe@googlemail.com>
+ *                      Zane Zaminsky <cyxman@yahoo.com>
+ *                      Jean-Pierre Rasquin <yank555.lu@gmail.com>
+ *                      ffolkes <ffolkes@ffolkes.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * -------------------------------------------------------------------------------------------------------------------------------------------------------
+ * -  Description:																	 -
+ * -------------------------------------------------------------------------------------------------------------------------------------------------------
+ *
+ * 'ZZMoove' governor is based on the modified 'conservative' (original author Alexander Clouter <alex@digriz.org.uk>) 'smoove' governor from Michael
+ * Weingaertner <mialwe@googlemail.com> (source: https://github.com/mialwe/mngb/) ported/modified/optimzed for I9300 since November 2012 and further
+ * improved for exynos and snapdragon platform (but also working on other platforms like OMAP) by ZaneZam,Yank555 and ffolkes in 2013/14/15/16
+ *
+ * --------------------------------------------------------------------------------------------------------------------------------------------------------
+ * -																			  -
+ * --------------------------------------------------------------------------------------------------------------------------------------------------------
+ */
+
+#include <linux/slab.h>
+#include "cpufreq_governor.h"
+
+// ZZ: for version information tunable
+#define ZZMOOVE_VERSION "bLE-develop"
+
+/* ZZMoove governor macros */
+#define DEF_FREQUENCY_UP_THRESHOLD		(80)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(40)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(10)
+#define DEF_SMOOTH_UP				(75)	// ZZ: default cpu load trigger for 'boosting' scaling frequency
+#define DEF_SCALING_PROPORTIONAL		(0)	// ZZ: default for proportional scaling, disabled here
+#define DEF_FAST_SCALING_UP			(0)	// Yank: default fast scaling for upscaling
+#define DEF_FAST_SCALING_DOWN			(0)	// Yank: default fast scaling for downscaling
+#define DEF_AFS_THRESHOLD1			(25)	// ZZ: default auto fast scaling step one
+#define DEF_AFS_THRESHOLD2			(50)	// ZZ: default auto fast scaling step two
+#define DEF_AFS_THRESHOLD3			(75)	// ZZ: default auto fast scaling step three
+#define DEF_AFS_THRESHOLD4			(90)	// ZZ: default auto fast scaling step four
+
+static DEFINE_PER_CPU(struct zz_cpu_dbs_info_s, zz_cpu_dbs_info);
+
+static DEFINE_PER_CPU(struct zz_dbs_tuners *, cached_tuners);
+
+static struct zz_governor_data {
+	unsigned int prev_load;
+} zz_data = {
+	.prev_load = 0,
+};
+
+// ZZ: function for frequency table order detection and limit optimization
+static inline void evaluate_scaling_order_limit_range(struct dbs_data *dbs_data)
+{
+	int freq_table_size = 0;
+	bool freq_table_desc = false;
+	unsigned int max_scaling_freq_hard = 0;
+	unsigned int max_scaling_freq_soft = 0;
+	unsigned int min_scaling_freq_hard = 0;
+	unsigned int min_scaling_freq = 0;
+	unsigned int limit_table_start = 0;
+	unsigned int limit_table_end = CPUFREQ_TABLE_END;
+	int i = 0;
+	int calc_index = 0;
+
+	// ZZ: initialisation of freq search in scaling table
+	for (i = 0; (likely(dbs_data->freq_table[i].frequency != CPUFREQ_TABLE_END)); i++) {
+		if (unlikely(dbs_data->pol_max == dbs_data->freq_table[i].frequency))
+			max_scaling_freq_hard = max_scaling_freq_soft = i;		// ZZ: init soft and hard max value
+		if (unlikely(dbs_data->pol_min == dbs_data->freq_table[i].frequency))
+			min_scaling_freq_hard = i;					// ZZ: init hard min value
+	/*
+	 * Yank: continue looping until table end is reached, 
+	 * we need this to set the table size limit below
+	 */
+	}
+
+	freq_table_size = i - 1;						// Yank: upper index limit of freq. table
+
+        /*
+         * ZZ: we have to take care about where we are in the frequency table. when using kernel sources without OC capability
+         * it might be that the first few indexes are containg no frequencies so a save index start point is needed.
+         */
+	calc_index = freq_table_size - max_scaling_freq_hard;			// ZZ: calculate the difference and use it as start point
+
+	if (calc_index == freq_table_size)					// ZZ: if we are at the end of the table
+		calc_index = calc_index - 1;					// ZZ: shift in range for order calculation below
+
+        // Yank: assert if CPU freq. table is in ascending or descending order
+	if (dbs_data->freq_table[calc_index].frequency > dbs_data->freq_table[calc_index+1].frequency) {
+		freq_table_desc = true;						// Yank: table is in descending order as expected, lowest freq at the bottom of the table
+		min_scaling_freq = i - 1;					// Yank: last valid frequency step (lowest frequency)
+		limit_table_start = max_scaling_freq_soft;			// ZZ: we should use the actual max scaling soft limit value as search start point
+        } else {
+		freq_table_desc = false;					// Yank: table is in ascending order, lowest freq at the top of the table
+		min_scaling_freq = 0;						// Yank: first valid frequency step (lowest frequency)
+		limit_table_start = min_scaling_freq_hard;			// ZZ: we should use the actual min scaling hard limit value as search start point
+		limit_table_end = dbs_data->pol_max;				// ZZ: end searching at highest frequency limit
+        }
+
+	// ZZ: save values in policy dbs structure
+	dbs_data->freq_table_desc = freq_table_desc;
+	dbs_data->freq_table_size = freq_table_size;
+	dbs_data->min_scaling_freq = min_scaling_freq;
+	dbs_data->limit_table_start = limit_table_start;
+	dbs_data->limit_table_end = limit_table_end;
+	dbs_data->max_scaling_freq_hard = max_scaling_freq_hard;
+	dbs_data->max_scaling_freq_soft = max_scaling_freq_soft;
+}
+
+// Yank: return a valid value between min and max
+static int validate_min_max(int val, int min, int max)
+{
+	return min(max(val, min), max);
+}
+
+// ZZ: system table scaling mode with freq search optimizations and proportional frequency target option
+static inline int zz_get_next_freq(unsigned int curfreq, unsigned int updown, unsigned int load, struct dbs_data *dbs_data)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	int i = 0;
+	unsigned int prop_target = 0;
+	unsigned int zz_target = 0;
+	unsigned int dead_band_freq = 0;					// ZZ: proportional freq, system table freq, dead band freq
+	int smooth_up_steps = 0;						// Yank: smooth up steps
+	int scaling_mode_up = 0;
+	int scaling_mode_down = 0;
+	static int tmp_limit_table_start = 0;
+	static int tmp_max_scaling_freq_soft = 0;
+	static int tmp_limit_table_end = 0;
+
+	prop_target = dbs_data->pol_min + load * (dbs_data->pol_max - dbs_data->pol_min) / 100;		// ZZ: prepare proportional target freq whitout deadband (directly mapped to min->max load)
+
+	if (zz_tuners->scaling_proportional == 2)				// ZZ: mode '2' use proportional target frequencies only
+	    return prop_target;
+
+	if (zz_tuners->scaling_proportional == 3) {				// ZZ: mode '3' use proportional target frequencies only and switch to pol_min in deadband range
+	    dead_band_freq = dbs_data->pol_max / 100 * load;			// ZZ: use old calculation to get deadband frequencies (=lower than pol_min)
+	    if (dead_band_freq > dbs_data->pol_min)				// ZZ: the load usually is too unsteady so we rarely would reach pol_min when load is low
+		return prop_target;						// ZZ: in fact it only will happen when load=0, so only return proportional frequencies if they
+	    else								//     are out of deadband range and if we are in deadband range return min freq
+		return dbs_data->pol_min;					//     (thats a similar behaving as with old propotional freq calculation)
+	}
+
+	if (load <= zz_tuners->smooth_up)					// Yank: consider smooth up
+	    smooth_up_steps = 0;						// Yank: load not reached, move by one step
+	else
+	    smooth_up_steps = 1;						// Yank: load reached, move by two steps
+
+	tmp_limit_table_start = dbs_data->limit_table_start;			// ZZ: first assign new limits...
+	tmp_limit_table_end = dbs_data->limit_table_end;
+	tmp_max_scaling_freq_soft = dbs_data->max_scaling_freq_soft;
+
+	// ZZ: asc: min freq limit changed
+	if (!dbs_data->freq_table_desc && curfreq
+	    < dbs_data->freq_table[dbs_data->min_scaling_freq].frequency)	// ZZ: asc: but reset starting index if current freq is lower than soft/hard min limit otherwise we are
+	    tmp_limit_table_start = 0;						//     shifting out of range and proportional freq is used instead because freq can't be found by loop
+
+	// ZZ: asc: max freq limit changed
+	if (!dbs_data->freq_table_desc && curfreq
+	    > dbs_data->freq_table[dbs_data->max_scaling_freq_soft].frequency)	// ZZ: asc: but reset ending index if current freq is higher than soft/hard max limit otherwise we are
+	    tmp_limit_table_end = dbs_data->freq_table[dbs_data->freq_table_size].frequency;	//     shifting out of range and proportional freq is used instead because freq can't be found by loop
+
+	// ZZ: desc: max freq limit changed
+	if (dbs_data->freq_table_desc && curfreq
+	    > dbs_data->freq_table[dbs_data->limit_table_start].frequency)	// ZZ: desc: but reset starting index if current freq is higher than soft/hard max limit otherwise we are
+	    tmp_limit_table_start = 0;						//     shifting out of range and proportional freq is used instead because freq can't be found by loop
+
+	// ZZ: feq search loop with optimization
+	if (dbs_data->freq_table_desc) {
+	    for (i = tmp_limit_table_start; (likely(dbs_data->freq_table[i].frequency != tmp_limit_table_end)); i++) {
+		if (unlikely(curfreq == dbs_data->freq_table[i].frequency)) {	// Yank: we found where we currently are (i)
+		    if (updown == 1) {						// Yank: scale up, but don't go above softlimit
+			zz_target = min(dbs_data->freq_table[tmp_max_scaling_freq_soft].frequency,
+		        dbs_data->freq_table[validate_min_max(i - 1 - smooth_up_steps - scaling_mode_up, 0, dbs_data->freq_table_size)].frequency);
+			if (zz_tuners->scaling_proportional == 1)		// ZZ: if proportional scaling is enabled
+			    return min(zz_target, prop_target);			// ZZ: check which freq is lower and return it
+			else
+			    return zz_target;					// ZZ: or return the found system table freq as usual
+		    } else {							// Yank: scale down, but don't go below min. freq.
+			zz_target = max(dbs_data->freq_table[dbs_data->min_scaling_freq].frequency,
+		        dbs_data->freq_table[validate_min_max(i + 1 + scaling_mode_down, 0, dbs_data->freq_table_size)].frequency);
+			if (zz_tuners->scaling_proportional == 1)		// ZZ: if proportional scaling is enabled
+			    return min(zz_target, prop_target);			// ZZ: check which freq is lower and return it
+			else
+			    return zz_target;					// ZZ: or return the found system table freq as usual
+		    }
+		}
+	    }									// ZZ: this shouldn't happen but if the freq is not found in system table
+	    return prop_target;							//     fall back to proportional freq target to avoid returning 0
+	} else {
+	    for (i = tmp_limit_table_start; (likely(dbs_data->freq_table[i].frequency <= tmp_limit_table_end)); i++) {
+		if (unlikely(curfreq == dbs_data->freq_table[i].frequency)) {	// Yank: we found where we currently are (i)
+		    if (updown == 1) {						// Yank: scale up, but don't go above softlimit
+			zz_target = min(dbs_data->freq_table[tmp_max_scaling_freq_soft].frequency,
+			dbs_data->freq_table[validate_min_max(i + 1 + smooth_up_steps + scaling_mode_up, 0, dbs_data->freq_table_size)].frequency);
+			if (zz_tuners->scaling_proportional == 1)		// ZZ: if proportional scaling is enabled
+			    return min(zz_target, prop_target);			// ZZ: check which freq is lower and return it
+			else
+			    return zz_target;					// ZZ: or return the found system table freq as usual
+		    } else {							// Yank: scale down, but don't go below min. freq.
+			zz_target = max(dbs_data->freq_table[dbs_data->min_scaling_freq].frequency,
+			dbs_data->freq_table[validate_min_max(i - 1 - scaling_mode_down, 0, dbs_data->freq_table_size)].frequency);
+			if (zz_tuners->scaling_proportional == 1)		// ZZ: if proportional scaling is enabled
+			    return min(zz_target, prop_target);			// ZZ: check which freq is lower and return it
+			else
+			    return zz_target;					// ZZ: or return the found system table freq as usual
+		    }
+		}
+	    }									// ZZ: this shouldn't happen but if the freq is not found in system table
+	    return prop_target;							//     fall back to proportional freq target to avoid returning 0
+	}
+}
+
+/*
+ * Every sampling_rate, we check, if current idle time is less than 20%
+ * (default), then we try to increase frequency. Every sampling_rate *
+ * sampling_down_factor, we check, if current idle time is more than 80%
+ * (default), then we try to decrease frequency
+ *
+ * Any frequency increase takes it to the maximum frequency. Frequency reduction
+ * happens at minimum steps of 5% (default) of maximum frequency
+ */
+static void zz_check_cpu(int cpu, unsigned int load)
+{
+	struct zz_cpu_dbs_info_s *dbs_info = &per_cpu(zz_cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy = dbs_info->cdbs.cur_policy;
+	struct dbs_data *dbs_data = policy->governor_data;
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+
+	/*
+	 * ZZ/Yank: Auto fast scaling mode
+	 * Switch to all 4 fast scaling modes depending on load gradient
+	 * the mode will start switching at given afs threshold load changes in both directions
+	 */
+	if (zz_tuners->fast_scaling_up       > 4) {
+	    if (load > zz_data.prev_load && load - zz_data.prev_load <= zz_tuners->afs_threshold1) {
+		dbs_data->scaling_mode_up = 0;
+	    } else if (load - zz_data.prev_load <= zz_tuners->afs_threshold2) {
+		dbs_data->scaling_mode_up = 1;
+	    } else if (load - zz_data.prev_load <= zz_tuners->afs_threshold3) {
+		dbs_data->scaling_mode_up = 2;
+	    } else if (load - zz_data.prev_load <= zz_tuners->afs_threshold4) {
+		dbs_data->scaling_mode_up = 3;
+	    } else {
+		dbs_data->scaling_mode_up = 4;
+	    }
+	}
+
+	if (zz_tuners->fast_scaling_down       > 4) {
+	  if (load < zz_data.prev_load && zz_data.prev_load - load <= zz_tuners->afs_threshold1) {
+		dbs_data->scaling_mode_down = 0;
+	    } else if (zz_data.prev_load - load <= zz_tuners->afs_threshold2) {
+		dbs_data->scaling_mode_down = 1;
+	    } else if (zz_data.prev_load - load <= zz_tuners->afs_threshold3) {
+		dbs_data->scaling_mode_down = 2;
+	    } else if (zz_data.prev_load - load <= zz_tuners->afs_threshold4) {
+		dbs_data->scaling_mode_down = 3;
+	    } else {
+		dbs_data->scaling_mode_down = 4;
+	    }
+	}
+
+	/* Check for frequency increase */
+	if (load > zz_tuners->up_threshold) {
+		dbs_info->down_skip = 0;
+
+		/* if we are already at full speed then break out early */
+		if (dbs_info->requested_freq == policy->max)
+			return;
+
+		dbs_info->requested_freq = zz_get_next_freq(policy->cur, 1, load, dbs_data);
+		
+		if (dbs_info->requested_freq > policy->max)
+			dbs_info->requested_freq = policy->max;
+
+		__cpufreq_driver_target(policy, dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+
+	/* if sampling_down_factor is active break out early */
+	if (++dbs_info->down_skip < zz_tuners->sampling_down_factor)
+		return;
+	dbs_info->down_skip = 0;
+
+	/* Check for frequency decrease */
+	if (load < zz_tuners->down_threshold) {
+		/*
+		 * if we cannot reduce the frequency anymore, break out early
+		 */
+		if (policy->cur == policy->min)
+			return;
+
+		dbs_info->requested_freq = zz_get_next_freq(policy->cur, 0, load, dbs_data);
+		
+		__cpufreq_driver_target(policy, dbs_info->requested_freq,
+				CPUFREQ_RELATION_L);
+		return;
+	}
+	zz_data.prev_load = load;
+}
+
+static void zz_dbs_timer(struct work_struct *work)
+{
+	struct zz_cpu_dbs_info_s *dbs_info = container_of(work,
+			struct zz_cpu_dbs_info_s, cdbs.work.work);
+	unsigned int cpu = dbs_info->cdbs.cur_policy->cpu;
+	struct zz_cpu_dbs_info_s *core_dbs_info = &per_cpu(zz_cpu_dbs_info,
+			cpu);
+	struct dbs_data *dbs_data = dbs_info->cdbs.cur_policy->governor_data;
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	int delay = delay_for_sampling_rate(zz_tuners->sampling_rate);
+	bool modify_all = true;
+
+	mutex_lock(&core_dbs_info->cdbs.timer_mutex);
+	if (!need_load_eval(&core_dbs_info->cdbs, zz_tuners->sampling_rate))
+		modify_all = false;
+	else
+		dbs_check_cpu(dbs_data, cpu);
+
+	gov_queue_work(dbs_data, dbs_info->cdbs.cur_policy, delay, modify_all);
+	mutex_unlock(&core_dbs_info->cdbs.timer_mutex);
+}
+
+static int dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct zz_cpu_dbs_info_s *dbs_info =
+					&per_cpu(zz_cpu_dbs_info, freq->cpu);
+	struct cpufreq_policy *policy;
+
+	if (!dbs_info->enable)
+		return 0;
+
+	policy = dbs_info->cdbs.cur_policy;
+
+	/*
+	 * we only care if our internally tracked freq moves outside the 'valid'
+	 * ranges of frequency available to us otherwise we do not change it
+	*/
+	if (dbs_info->requested_freq > policy->max
+			|| dbs_info->requested_freq < policy->min)
+		dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+/************************** sysfs interface ************************/
+static struct common_dbs_data zz_dbs_cdata;
+
+static ssize_t store_sampling_down_factor(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	zz_tuners->sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	zz_tuners->sampling_rate = max(input, dbs_data->min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input <= zz_tuners->down_threshold)
+		return -EINVAL;
+
+	zz_tuners->up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_threshold(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	/* cannot be lower than 11 otherwise freq will not fall */
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= zz_tuners->up_threshold)
+		return -EINVAL;
+
+	zz_tuners->down_threshold = input;
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == zz_tuners->ignore_nice_load) /* nothing to do */
+		return count;
+
+	zz_tuners->ignore_nice_load = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct zz_cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(zz_cpu_dbs_info, j);
+		dbs_info->cdbs.prev_cpu_idle = get_cpu_idle_time(j,
+					&dbs_info->cdbs.prev_cpu_wall, 0);
+		if (zz_tuners->ignore_nice_load)
+			dbs_info->cdbs.prev_cpu_nice =
+				kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+// ZZ: tuneable -> possible values: range from 1 to 100, if not set default is 75
+static ssize_t store_smooth_up(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > 100 || input < 1)
+	    return -EINVAL;
+
+	zz_tuners->smooth_up = input;
+
+	return count;
+}
+
+/*
+ * ZZ: tuneable scaling proportinal -> possible values: 0 to disable, 
+ * 1 to enable comparision between proportional and optimized freq, 
+ * 2 to enable propotional freq usage only
+ * 3 to enable propotional freq usage only but with dead brand range 
+ * to avoid not reaching of pol min freq, 
+ * if not set default is 0
+ */
+static ssize_t store_scaling_proportional(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || input > 3)
+	    return -EINVAL;
+
+	zz_tuners->scaling_proportional = input;
+
+	return count;
+}
+
+/*
+ * Yank: tuneable -> possible values 1-4 to enable fast scaling 
+ * and 5 for auto fast scaling (insane scaling)
+ */
+static ssize_t store_fast_scaling_up(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 5 || input < 0)
+	    return -EINVAL;
+
+	zz_tuners->fast_scaling_up = input;
+
+	if (input > 4)				// ZZ: auto fast scaling mode
+	    return count;
+
+	dbs_data->scaling_mode_up = input;	// Yank: fast scaling up only
+
+	return count;
+}
+
+/*
+* Yank: tuneable -> possible values 1-4 to enable fast scaling 
+* and 5 for auto fast scaling (insane scaling)
+*/
+static ssize_t store_fast_scaling_down(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 5 || input < 0)
+	    return -EINVAL;
+
+	zz_tuners->fast_scaling_down = input;
+
+	if (input > 4)				// ZZ: auto fast scaling mode
+	    return count;
+
+	dbs_data->scaling_mode_down = input;	// Yank: fast scaling up only
+
+	return count;
+}
+
+// ZZ: afs tuneable -> possible values from 0 to 100
+#define store_afs_threshold(name)					\
+static ssize_t store_afs_threshold##name(struct dbs_data *dbs_data,	\
+		const char *buf, size_t count)				\
+{									\
+	struct zz_dbs_tuners *zz_tuners = dbs_data->tuners;		\
+	unsigned int input;						\
+	int ret;							\
+									\
+	ret = sscanf(buf, "%u", &input);				\
+									\
+	if (ret != 1 || input > 100 || input < 0)			\
+	    return -EINVAL;						\
+									\
+	zz_tuners->afs_threshold##name = input;				\
+									\
+	return count;							\
+}									\
+
+store_afs_threshold(1);
+store_afs_threshold(2);
+store_afs_threshold(3);
+store_afs_threshold(4);
+
+// ZZ: show zzmoove version info in sysfs
+#define declare_show_version(_gov)					\
+static ssize_t show_version_gov_sys					\
+	(struct kobject *kobj, struct attribute *attr, char *buf)	\
+{									\
+	return sprintf(buf, "%s\n", ZZMOOVE_VERSION);			\
+}									\
+									\
+static ssize_t show_version_gov_pol					\
+	(struct cpufreq_policy *policy, char *buf)			\
+{									\
+	return sprintf(buf, "%s\n", ZZMOOVE_VERSION);			\
+}
+
+show_store_one(zz, sampling_rate);
+show_store_one(zz, sampling_down_factor);
+show_store_one(zz, up_threshold);
+show_store_one(zz, down_threshold);
+show_store_one(zz, ignore_nice_load);
+show_store_one(zz, smooth_up);
+show_store_one(zz, scaling_proportional);
+show_store_one(zz, fast_scaling_up);
+show_store_one(zz, fast_scaling_down);
+show_store_one(zz, afs_threshold1);
+show_store_one(zz, afs_threshold2);
+show_store_one(zz, afs_threshold3);
+show_store_one(zz, afs_threshold4);
+declare_show_version(zz);
+declare_show_sampling_rate_min(zz);
+
+gov_sys_pol_attr_rw(sampling_rate);
+gov_sys_pol_attr_rw(sampling_down_factor);
+gov_sys_pol_attr_rw(up_threshold);
+gov_sys_pol_attr_rw(down_threshold);
+gov_sys_pol_attr_rw(ignore_nice_load);
+gov_sys_pol_attr_rw(smooth_up);
+gov_sys_pol_attr_rw(scaling_proportional);
+gov_sys_pol_attr_rw(fast_scaling_up);
+gov_sys_pol_attr_rw(fast_scaling_down);
+gov_sys_pol_attr_rw(afs_threshold1);
+gov_sys_pol_attr_rw(afs_threshold2);
+gov_sys_pol_attr_rw(afs_threshold3);
+gov_sys_pol_attr_rw(afs_threshold4);
+gov_sys_pol_attr_ro(version);
+gov_sys_pol_attr_ro(sampling_rate_min);
+
+static struct attribute *dbs_attributes_gov_sys[] = {
+	&version_gov_sys.attr,
+	&sampling_rate_min_gov_sys.attr,
+	&sampling_rate_gov_sys.attr,
+	&sampling_down_factor_gov_sys.attr,
+	&up_threshold_gov_sys.attr,
+	&down_threshold_gov_sys.attr,
+	&ignore_nice_load_gov_sys.attr,
+	&smooth_up_gov_sys.attr,
+	&scaling_proportional_gov_sys.attr,
+	&fast_scaling_up_gov_sys.attr,
+	&fast_scaling_down_gov_sys.attr,
+	&afs_threshold1_gov_sys.attr,
+	&afs_threshold2_gov_sys.attr,
+	&afs_threshold3_gov_sys.attr,
+	&afs_threshold4_gov_sys.attr,
+	NULL
+};
+
+static struct attribute_group zz_attr_group_gov_sys = {
+	.attrs = dbs_attributes_gov_sys,
+	.name = "zzmoove",
+};
+
+static struct attribute *dbs_attributes_gov_pol[] = {
+	&version_gov_pol.attr,
+	&sampling_rate_min_gov_pol.attr,
+	&sampling_rate_gov_pol.attr,
+	&sampling_down_factor_gov_pol.attr,
+	&up_threshold_gov_pol.attr,
+	&down_threshold_gov_pol.attr,
+	&ignore_nice_load_gov_pol.attr,
+	&smooth_up_gov_pol.attr,
+	&scaling_proportional_gov_pol.attr,
+	&fast_scaling_up_gov_pol.attr,
+	&fast_scaling_down_gov_pol.attr,
+	&afs_threshold1_gov_pol.attr,
+	&afs_threshold2_gov_pol.attr,
+	&afs_threshold3_gov_pol.attr,
+	&afs_threshold4_gov_pol.attr,
+	NULL
+};
+
+static struct attribute_group zz_attr_group_gov_pol = {
+	.attrs = dbs_attributes_gov_pol,
+	.name = "zzmoove",
+};
+
+/************************** sysfs end ************************/
+
+static void save_tuners(struct cpufreq_policy *policy,
+		struct zz_dbs_tuners *tuners)
+{
+	int cpu;
+
+	if (have_governor_per_policy())
+	    cpu = cpumask_first(policy->related_cpus);
+	else
+	    cpu = 0;
+
+	WARN_ON(per_cpu(cached_tuners, cpu) &&
+	per_cpu(cached_tuners, cpu) != tuners);
+	per_cpu(cached_tuners, cpu) = tuners;
+}
+
+static struct zz_dbs_tuners *alloc_tuners(struct cpufreq_policy *policy)
+{
+	struct zz_dbs_tuners *tuners;
+
+	tuners = kzalloc(sizeof(*tuners), GFP_KERNEL);
+	if (!tuners) {
+	    pr_err("%s: kzalloc failed\n", __func__);
+	    return ERR_PTR(-ENOMEM);
+	}
+
+	tuners->up_threshold = DEF_FREQUENCY_UP_THRESHOLD;
+	tuners->down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD;
+	tuners->sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR;
+	tuners->ignore_nice_load = 0;
+	tuners->smooth_up = DEF_SMOOTH_UP;
+	tuners->scaling_proportional = DEF_SCALING_PROPORTIONAL;
+	tuners->fast_scaling_up = DEF_FAST_SCALING_UP;
+	tuners->fast_scaling_down = DEF_FAST_SCALING_DOWN;
+	tuners->afs_threshold1 = DEF_AFS_THRESHOLD1;
+	tuners->afs_threshold2 = DEF_AFS_THRESHOLD2;
+	tuners->afs_threshold3 = DEF_AFS_THRESHOLD3;
+	tuners->afs_threshold4 = DEF_AFS_THRESHOLD4;
+
+	save_tuners(policy, tuners);
+
+	return tuners;
+}
+
+static struct zz_dbs_tuners *restore_tuners(struct cpufreq_policy *policy)
+{
+	int cpu;
+
+	if (have_governor_per_policy())
+	    cpu = cpumask_first(policy->related_cpus);
+	else
+	    cpu = 0;
+
+	return per_cpu(cached_tuners, cpu);
+}
+
+static int zz_init(struct dbs_data *dbs_data, struct cpufreq_policy *policy)
+{
+	struct zz_dbs_tuners *tuners;
+
+	tuners = restore_tuners(policy);
+	if (!tuners) {
+		tuners = alloc_tuners(policy);
+		if (IS_ERR(tuners))
+			return PTR_ERR(tuners);
+	}
+
+	dbs_data->tuners = tuners;
+	dbs_data->min_sampling_rate = MIN_SAMPLING_RATE_RATIO *
+		jiffies_to_usecs(10);
+	dbs_data->freq_table = cpufreq_frequency_get_table(policy->cpu);
+	dbs_data->pol_min = policy->min;
+	dbs_data->pol_max = policy->max;
+	evaluate_scaling_order_limit_range(dbs_data);
+	mutex_init(&dbs_data->mutex);
+	return 0;
+}
+
+static void zz_exit(struct dbs_data *dbs_data)
+{
+//	nothing to do
+}
+
+define_get_cpu_dbs_routines(zz_cpu_dbs_info);
+
+static struct notifier_block zz_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier,
+};
+
+static struct zz_ops zz_ops = {
+	.notifier_block = &zz_cpufreq_notifier_block,
+};
+
+static struct common_dbs_data zz_dbs_cdata = {
+	.governor = GOV_ZZMOOVE,
+	.attr_group_gov_sys = &zz_attr_group_gov_sys,
+	.attr_group_gov_pol = &zz_attr_group_gov_pol,
+	.get_cpu_cdbs = get_cpu_cdbs,
+	.get_cpu_dbs_info_s = get_cpu_dbs_info_s,
+	.gov_dbs_timer = zz_dbs_timer,
+	.gov_check_cpu = zz_check_cpu,
+	.gov_ops = &zz_ops,
+	.init_zz = zz_init,
+	.exit = zz_exit,
+};
+
+static int zz_cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	return cpufreq_governor_dbs(policy, &zz_dbs_cdata, event);
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_zzmoove = {
+	.name			= "zzmoove",
+	.governor		= zz_cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_zzmoove);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	int cpu;
+
+	cpufreq_unregister_governor(&cpufreq_gov_zzmoove);
+	for_each_possible_cpu(cpu) {
+		kfree(per_cpu(cached_tuners, cpu));
+		per_cpu(cached_tuners, cpu) = NULL;
+	}
+}
+
+MODULE_AUTHOR("Zane Zaminsky <cyxman@yahoo.com>");
+MODULE_DESCRIPTION("'cpufreq_zzmoove' - A dynamic cpufreq governor based "
+	"on smoove governor from Michael Weingaertner which was originally based on "
+	"conservative governor from Alexander Clouter. Optimized for use with Samsung I9300 "
+	"using a fast scaling logic - ported/modified/optimized for I9300 since November 2012 "
+	"and further improved for exynos and snapdragon platform "
+	"by ZaneZam,Yank555 and ffolkes in 2013/14/15/16");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
